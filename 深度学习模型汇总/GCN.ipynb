{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "077a6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh, ArpackNoConvergence\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.initializers import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import activations, initializers, constraints\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3a512ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data_path = 'cora/cora.content'\n",
    "edge_data_path = 'cora/cora.cites'\n",
    "\n",
    "# 数据集链接：https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
    "idx_features_labels = np.genfromtxt(node_data_path, dtype=np.dtype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "928a14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "labels = encode_onehot(idx_features_labels[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b2d338e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(adj) has 2708 nodes, 5429 edges, 1433 features.\n"
     ]
    }
   ],
   "source": [
    "# build graph\n",
    "idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "idx_map = {j: i for i, j in enumerate(idx)}\n",
    "\n",
    "edges_unordered = np.genfromtxt(edge_data_path, dtype=np.int32)\n",
    "\n",
    "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                 dtype=np.int32).reshape(edges_unordered.shape)\n",
    "\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                    shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "# build symmetric adjacency matrix\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "print('Dataset(adj) has {} nodes, {} edges, {} features.'.format(adj.shape[0], edges.shape[0], features.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51aa70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = features.todense()\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60ef23ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = adj.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fe299b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2708"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23c993f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x /= x.sum(1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f04526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18438\\AppData\\Local\\Temp/ipykernel_17392/2324728083.py:4: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array(mask, dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "def sample_mask(idx, l):\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def get_splits(y):\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "    y_train = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_val = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_test = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_train[idx_train] = y[idx_train]\n",
    "    y_val[idx_val] = y[idx_val]\n",
    "    y_test[idx_test] = y[idx_test]\n",
    "    train_mask = sample_mask(idx_train, y.shape[0])\n",
    "    return y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask\n",
    "\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "011513d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    filter = 'localpool'    # Local pooling filters (see 'renormalization trick' in Kipf & Welling, arXiv 2016)\n",
    "    # filter = 'chebyshev'  # Chebyshev polynomial basis filters (Defferard et al., NIPS 2016)\n",
    "    max_degree = 2  # maximum polynomial degree\n",
    "    sym_norm = True  # symmetric (True) vs. left-only (False) normalization\n",
    "    NB_EPOCH = 20\n",
    "    PATIENCE = 10  # early stopping patience\n",
    "    support = 1\n",
    "    epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01b0945e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local pooling filters...\n"
     ]
    }
   ],
   "source": [
    "def preprocess_adj(adj, symmetric=True):\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    adj = normalize_adj(adj, symmetric)\n",
    "    return adj\n",
    "\n",
    "def normalize_adj(adj, symmetric=True):\n",
    "    if symmetric:\n",
    "        d = sp.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
    "        a_norm = adj.dot(d).transpose().dot(d).tocsr()\n",
    "    else:\n",
    "        d = sp.diags(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
    "        a_norm = d.dot(adj).tocsr()\n",
    "    return a_norm\n",
    "\n",
    "def get_inputs(adj, x):\n",
    "    if Config.filter == 'localpool':\n",
    "        print('Using local pooling filters...')\n",
    "        adj_ = preprocess_adj(adj, Config.sym_norm)\n",
    "        adj_ = adj_.todense()\n",
    "        graph = [x, adj_]\n",
    "        adj_input = [Input(batch_shape=(None, None), sparse=False, name='adj_input')]\n",
    "    elif Config.filter == 'chebyshev':\n",
    "        print('Using Chebyshev polynomial basis filters...')\n",
    "        L = normalized_laplacian(adj, Config.sym_norm)\n",
    "        L_scaled = rescale_laplacian(L)\n",
    "        T_k = chebyshev_polynomial(L_scaled, Config.max_degree)\n",
    "        support = Config.max_degree + 1\n",
    "        graph = [x] + T_k\n",
    "        adj_input = [Input(batch_shape=(None, None), sparse=False, name='adj_input') for _ in range(support)]\n",
    "    else:\n",
    "        raise Exception('Invalid filter type.')\n",
    "    return graph, adj_input\n",
    "\n",
    "x_graph, adj_input = get_inputs(adj, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "639e219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Basic graph convolution layer as in https://arxiv.org/abs/1609.02907\"\"\"\n",
    "    def __init__(self, units, support=1,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.supports_masking = True\n",
    "        self.support = support\n",
    "        assert support >= 1.0\n",
    "\n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        features_shape = input_shapes[0]\n",
    "        output_shape = (features_shape[0], self.units)\n",
    "        return output_shape  # (batch_size, output_dim)\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        features_shape = input_shapes[0]\n",
    "        assert len(features_shape) == 2\n",
    "        input_dim = features_shape[1]\n",
    "        self.kernel = self.add_weight(shape=(input_dim * self.support,\n",
    "                                             self.units),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.built = True\n",
    "    \n",
    "    # core code\n",
    "    def call(self, inputs, mask=None):\n",
    "        features = inputs[0]\n",
    "        basis = inputs[1:] # this is a list\n",
    "        supports = list()\n",
    "        for i in range(self.support):\n",
    "            # A * X\n",
    "            supports.append(K.dot(basis[i], features))\n",
    "        supports = K.concatenate(supports, axis=1)\n",
    "        # A * X * W\n",
    "        output = K.dot(supports, self.kernel)\n",
    "        if tf.is_tensor(self.bias) :\n",
    "            output += self.bias\n",
    "        return self.activation(output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "                  'support': self.support,\n",
    "                  'activation': activations.serialize(self.activation),\n",
    "                  'use_bias': self.use_bias,\n",
    "                  'kernel_initializer': initializers.serialize(\n",
    "                      self.kernel_initializer),\n",
    "                  'bias_initializer': initializers.serialize(\n",
    "                      self.bias_initializer),\n",
    "                  'kernel_regularizer': regularizers.serialize(\n",
    "                      self.kernel_regularizer),\n",
    "                  'bias_regularizer': regularizers.serialize(\n",
    "                      self.bias_regularizer),\n",
    "                  'activity_regularizer': regularizers.serialize(\n",
    "                      self.activity_regularizer),\n",
    "                  'kernel_constraint': constraints.serialize(\n",
    "                      self.kernel_constraint),\n",
    "                  'bias_constraint': constraints.serialize(self.bias_constraint)\n",
    "        }\n",
    "        base_config = super(GraphConvolution, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "defb1fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x, y, adj_input):\n",
    "    fea_input = Input(batch_shape=(None, x.shape[1]), name='fea_input')\n",
    "    net = Dropout(0.2)(fea_input)\n",
    "    net = GraphConvolution(128, Config.support, activation='relu', kernel_regularizer=l2(5e-4))([net] + adj_input)\n",
    "    net = Dropout(0.2)(net)\n",
    "    net = GraphConvolution(64, Config.support, activation='relu', kernel_regularizer=l2(5e-4))([net] + adj_input)\n",
    "    net = Dropout(0.2)(net)\n",
    "    net = Flatten()(net)\n",
    "    output = Dense(y.shape[1], activation='softmax')(net)\n",
    "    # output = GraphConvolution(y.shape[1], Config.support, activation='softmax')([net] + adj_input)\n",
    "    model = Model(inputs=[fea_input] + adj_input, outputs=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3afb7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(x, y, adj_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "adfe7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    return np.mean(np.equal(np.argmax(labels, 1), np.argmax(preds, 1)))\n",
    "\n",
    "\n",
    "def evaluate_preds(preds, labels, indices):\n",
    "    split_loss = list()\n",
    "    split_acc = list()\n",
    "    for y_split, idx_split in zip(labels, indices):\n",
    "        split_loss.append(categorical_crossentropy(preds[idx_split], y_split[idx_split]))\n",
    "        split_acc.append(accuracy(preds[idx_split], y_split[idx_split]))\n",
    "    return split_loss, split_acc\n",
    "\n",
    "\n",
    "def categorical_crossentropy(preds, labels):\n",
    "    return np.mean(-np.log(np.extract(labels, preds)))\n",
    "\n",
    "\n",
    "def train_model(x, y, model, train_mask, y_train, y_val, idx_train, idx_val, batch_size):\n",
    "    for i in range(Config.epochs):\n",
    "        model.fit(x, y, sample_weight=train_mask, batch_size=batch_size, epochs=1, shuffle=False, verbose=1)\n",
    "        y_pred = model.predict(x, batch_size=batch_size)\n",
    "        train_val_loss, train_val_acc = evaluate_preds(y_pred, [y_train, y_val], [idx_train, idx_val])\n",
    "        print(\"train_loss= {:.2f}\".format(train_val_loss[0]), \"train_acc= {:.2f}\".format(train_val_acc[0]),\n",
    "              \"val_loss= {:.2f}\".format(train_val_loss[1]), \"val_acc= {:.2f}\".format(train_val_acc[1]))\n",
    "    return model\n",
    "\n",
    "\n",
    "def estimate_model(model, x, y_test, idx_test, batch_size):\n",
    "    y_pred = model.predict(x, batch_size=batch_size)\n",
    "    test_loss, test_acc = evaluate_preds(y_pred, [y_test], [idx_test])\n",
    "    print(\"Test set results:\", \"loss= {:.2f}\".format(test_loss[0]), \"accuracy= {:.4f}\".format(test_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5928ffe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 996us/step - loss: 0.0969\n",
      "train_loss= 1.80 train_acc= 0.31 val_loss= 1.80 val_acc= 0.35\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0965\n",
      "train_loss= 1.79 train_acc= 0.31 val_loss= 1.80 val_acc= 0.35\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0965\n",
      "train_loss= 1.79 train_acc= 0.29 val_loss= 1.80 val_acc= 0.34\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0976\n",
      "train_loss= 1.79 train_acc= 0.31 val_loss= 1.80 val_acc= 0.35\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0976\n",
      "train_loss= 1.79 train_acc= 0.31 val_loss= 1.80 val_acc= 0.34\n"
     ]
    }
   ],
   "source": [
    "model = train_model(x_graph, y, model, train_mask, y_train, y_val, idx_train, idx_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b52c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78395320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2x",
   "language": "python",
   "name": "tf2x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
